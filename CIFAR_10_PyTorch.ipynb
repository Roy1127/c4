{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR-10 PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TRTRd5QxKCNR",
        "ZJPCEllBKGVC",
        "EzxCqfgxMvbq"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roy1127/c4/blob/master/CIFAR_10_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRTRd5QxKCNR"
      },
      "source": [
        "####Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOOVlA94Kwhz"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cifar10(model, dataloader, n_epochs=1, optimizer=None,\n",
        "          lr=0.001, weight_decay=0.001, loss_fn=None, isPytorch=True):\n",
        "    np.random.seed(4)\n",
        "    opt = optimizer(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        start = time.time()\n",
        "        avg_acc, avg_loss = 0, 0\n",
        "        if isPytorch:\n",
        "          avg_acc, avg_loss = epoch_pytorch_cifar10(dataloader, model, loss_fn=loss_fn, opt=opt)\n",
        "        else:\n",
        "          avg_acc, avg_loss = epoch_general_cifar10(dataloader, model, loss_fn=loss_fn, opt=opt)\n",
        "        epoch_t = time.time() - start\n",
        "        print(f\"Epoch: {epoch}, Acc: {avg_acc}, Loss: {avg_loss}, Time: {epoch_t:.2f}s\")\n",
        "\n",
        "    return avg_acc, avg_loss\n",
        "\n",
        "def epoch_pytorch_cifar10(dataloader, model, loss_fn, opt=None):\n",
        "    train_loss = 0.0\n",
        "    correct = 0.0\n",
        "\n",
        "    # training model loop\n",
        "    model.train()\n",
        "    for data, labels in dataloader:\n",
        "\n",
        "      if train_on_gpu:\n",
        "        data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "      opt.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = loss_fn(output, labels)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      train_loss += loss.item() * data.size(0)\n",
        "      _, predicted = torch.max(output, 1)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct/len(dataloader.dataset), train_loss/len(dataloader.dataset)\n",
        "\n",
        "def epoch_general_cifar10(dataloader, model, loss_fn=nn.SoftmaxLoss(), opt=None):\n",
        "    np.random.seed(4)\n",
        "    correct, total_loss = 0, 0\n",
        "\n",
        "    if opt is None:\n",
        "        model.eval()\n",
        "        for batch in dataloader:\n",
        "            X, y = batch\n",
        "            X, y = ndl.Tensor(X, device=device), ndl.Tensor(y, device=device)\n",
        "            out = model(X)\n",
        "            loss = loss_fn(out, y)\n",
        "            correct += np.sum(np.argmax(out.numpy(), axis=1) == y.numpy())\n",
        "            total_loss += loss.data.numpy() * y.shape[0]\n",
        "    else:\n",
        "        model.train()\n",
        "        for batch in dataloader:\n",
        "            opt.reset_grad()\n",
        "            X, y = batch\n",
        "            X, y = ndl.Tensor(X, device=device), ndl.Tensor(y, device=device)\n",
        "            out = model(X)\n",
        "            loss = loss_fn(out, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            correct += np.sum(np.argmax(out.numpy(), axis=1) == y.numpy())\n",
        "            total_loss += loss.data.numpy() * y.shape[0]\n",
        "\n",
        "    sample_nums = len(dataloader.dataset)\n",
        "    return correct / sample_nums, total_loss / sample_nums"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "E5TUHH14Tx4g",
        "outputId": "4b54c0fc-5491-45bb-886f-7f094dd58d47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'torch.nn' has no attribute 'SoftmaxLoss'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c14131aa7e92>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mepoch_general_cifar10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmaxLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'SoftmaxLoss'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJPCEllBKGVC"
      },
      "source": [
        "####Check for GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hW4lB08K2EN",
        "outputId": "d9104961-dc4b-40a0-d0dd-dc1ba2846435",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('PyTorch version:', torch.__version__)\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.0+cu121\n",
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlzJAQ3jLnRt"
      },
      "source": [
        "# Set random seed for reproducability\n",
        "torch.manual_seed(271828)\n",
        "np.random.seed(271728)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PyTorch Model for Cifar10"
      ],
      "metadata": {
        "id": "qwRBmuJVUbts"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPjns4ErHkfX"
      },
      "source": [
        "####Data Preparation for PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y0XCJ6sLJe_",
        "outputId": "fe7e8494-4468-4380-bd2c-e1cb53ee0a49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# transform for the training data\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.1307], [0.3081])\n",
        "])\n",
        "\n",
        "# load datasets, downloading if needed\n",
        "train_set = CIFAR10('./data/cifar10', train=True, download=True,\n",
        "                  transform=train_transform)\n",
        "\n",
        "train_len = len(train_set)\n",
        "idxes = list(range(train_len))\n",
        "\n",
        "#shuffle the indexes\n",
        "np.random.shuffle(idxes)\n",
        "\n",
        "split = int(np.floor(0.2*train_len))\n",
        "train_idx, valid_idx = idxes[split: ], idxes[:split]\n",
        "\n",
        "#Sampler gets the batches of data\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_len = len(train_set)\n",
        "idxes = list(range(train_len))\n",
        "\n",
        "#shuffle the indexes\n",
        "np.random.shuffle(idxes)\n",
        "\n",
        "split = int(np.floor(0.2*train_len))\n",
        "train_idx, valid_idx = idxes[split: ], idxes[:split]\n",
        "\n",
        "#Sampler gets the batches of data\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=20, sampler=train_sampler, num_workers=0)\n",
        "valid_loader = torch.utils.data.DataLoader(train_set, batch_size=20, sampler=valid_sampler, num_workers=0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 76.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTZM9tlcXVET"
      },
      "source": [
        "###Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "              nn.BatchNorm2d(out_channels),\n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResNet9(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = conv_block(in_channels, 64)\n",
        "        self.conv2 = conv_block(64, 128, pool=True)\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "\n",
        "        self.conv3 = conv_block(128, 256, pool=True)\n",
        "        self.conv4 = conv_block(256, 512, pool=True)\n",
        "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
        "\n",
        "        self.classifier = nn.Sequential(nn.AdaptiveMaxPool2d((1,1)),\n",
        "                                        nn.Flatten(),\n",
        "                                        nn.Dropout(0.2),\n",
        "                                        nn.Linear(512, num_classes))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "model = ResNet9(3, 10)\n",
        "print(model)\n",
        "\n",
        "if train_on_gpu:\n",
        "  model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWOn6YpyZmLu",
        "outputId": "ce1ffe0d-9a22-4d1d-dcc1-0496f77fa04b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet9(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (res1): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (res2): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): AdaptiveMaxPool2d(output_size=(1, 1))\n",
            "    (1): Flatten(start_dim=1, end_dim=-1)\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "DVngbKycgLw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cifar10(model, train_loader, n_epochs=10, optimizer=optim.Adam, lr=1e-4, loss_fn=nn.CrossEntropyLoss())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtjGDBSApGY8",
        "outputId": "ce9c5f6b-346d-4a66-b14d-6b9edb7cb19e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Acc: 0.41844, Loss: 1.101134807085991, Time: 39.75s\n",
            "Epoch: 1, Acc: 0.5559, Loss: 0.6986673478364944, Time: 39.29s\n",
            "Epoch: 2, Acc: 0.61686, Loss: 0.5276486627548933, Time: 39.44s\n",
            "Epoch: 3, Acc: 0.65254, Loss: 0.4206232277840376, Time: 39.08s\n",
            "Epoch: 4, Acc: 0.68348, Loss: 0.3356303848192096, Time: 39.39s\n",
            "Epoch: 5, Acc: 0.70388, Loss: 0.2789133390113711, Time: 39.23s\n",
            "Epoch: 6, Acc: 0.7238, Loss: 0.2222105843245983, Time: 39.37s\n",
            "Epoch: 7, Acc: 0.73588, Loss: 0.18663425630070268, Time: 39.44s\n",
            "Epoch: 8, Acc: 0.74752, Loss: 0.15300494082830846, Time: 39.38s\n",
            "Epoch: 9, Acc: 0.75538, Loss: 0.1315854042828083, Time: 39.49s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.75538, 0.1315854042828083)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Custom Model for Cifar10"
      ],
      "metadata": {
        "id": "abS6FR0oUFl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to set up the assignment\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/\n",
        "!mkdir -p 10714\n",
        "%cd /content/drive/MyDrive/10714\n",
        "!git clone https://github.com/dlsys10714/hw4.git\n",
        "%cd /content/drive/MyDrive/10714/hw4\n",
        "\n",
        "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
        "!pip3 install pybind11\n",
        "\n",
        "%set_env PYTHONPATH ./python\n",
        "%set_env NEEDLE_BACKEND nd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFYgc9wahSco",
        "outputId": "a3cddf15-c4ad-44c9-f0c1-b3f21ca440af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/10714\n",
            "fatal: destination path 'hw4' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/10714/hw4\n",
            "Collecting git+https://github.com/dlsys10714/mugrade.git\n",
            "  Cloning https://github.com/dlsys10714/mugrade.git to /tmp/pip-req-build-z5jgb75p\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/dlsys10714/mugrade.git /tmp/pip-req-build-z5jgb75p\n",
            "  Resolved https://github.com/dlsys10714/mugrade.git to commit 656cdc2b7ad5a37e7a5347a7b0405df0acd72380\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mugrade\n",
            "  Building wheel for mugrade (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mugrade: filename=mugrade-1.2-py3-none-any.whl size=3935 sha256=6ae1be0469d2de02f0d6bddb00b6f5a06b0abb7c4e0069ef3b2845f812097cb8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f5veq90o/wheels/8b/ba/3a/621da1207eab160c01968c5e0bd1266f505b9e3f8010376d61\n",
            "Successfully built mugrade\n",
            "Installing collected packages: mugrade\n",
            "Successfully installed mugrade-1.2\n",
            "Collecting pybind11\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybind11\n",
            "Successfully installed pybind11-2.13.6\n",
            "env: PYTHONPATH=./python\n",
            "env: NEEDLE_BACKEND=nd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation for Custom Framework"
      ],
      "metadata": {
        "id": "EbK6Iu-MgcCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('./python')\n",
        "sys.path.append('./apps')\n",
        "import needle as ndl\n",
        "from models import ResNet9\n",
        "from simple_training import train_cifar10\n",
        "\n",
        "device = ndl.cuda()\n",
        "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
        "dataloader = ndl.data.DataLoader(\\\n",
        "         dataset=dataset,\n",
        "         batch_size=128,\n",
        "         shuffle=True,\n",
        "         collate_fn=ndl.data.collate_ndarray,\n",
        "         device=device,\n",
        "         dtype=\"float32\")"
      ],
      "metadata": {
        "id": "sxtmX4xTjrWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model"
      ],
      "metadata": {
        "id": "-2hKoFBTgi1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class ResNet9(ndl.nn.Module):\n",
        "#     def __init__(self, device=None, dtype=\"float32\"):\n",
        "#         super().__init__()\n",
        "#         ### BEGIN YOUR SOLUTION ###\n",
        "#         self.model = nn.Sequential(ConvBNBlock(3, 16, 7, 4, device=device),\n",
        "#                                    ConvBNBlock(16, 32, 3, 2, device=device),\n",
        "#                                    nn.Residual(nn.Sequential(ConvBNBlock(32, 32, 3, 1, device=device),\n",
        "#                                                              ConvBNBlock(32, 32, 3, 1, device=device))),\n",
        "#                                    ConvBNBlock(32, 64, 3, 2, device=device),\n",
        "#                                    ConvBNBlock(64, 128, 3, 2, device=device),\n",
        "#                                    nn.Residual(nn.Sequential(ConvBNBlock(128, 128, 3, 1, device=device),\n",
        "#                                                              ConvBNBlock(128, 128, 3, 1, device=device))),\n",
        "#                                    nn.Flatten(),\n",
        "#                                    nn.Linear(128, 128, device=device),\n",
        "#                                    nn.ReLU(),\n",
        "#                                    nn.Linear(128, 10, device=device))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.model(x)\n",
        "\n",
        "model = ResNet9(device=device, dtype=\"float32\")"
      ],
      "metadata": {
        "id": "Xrn5COE6k5vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "h76Q5sFOgnD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cifar10(model, dataloader, n_epochs=10, optimizer=ndl.optim.Adam,\n",
        "      lr=0.001, weight_decay=0.001)"
      ],
      "metadata": {
        "id": "lowzAQ1ck-oH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}