{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR-10 PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TRTRd5QxKCNR",
        "ZJPCEllBKGVC",
        "EzxCqfgxMvbq"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roy1127/c4/blob/master/CIFAR_10_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRTRd5QxKCNR"
      },
      "source": [
        "####Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "ylEvangnnhQF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOOVlA94Kwhz"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cifar10(model, dataloader, n_epochs=1, optimizer=None,\n",
        "          lr=0.001, weight_decay=0.001, loss_fn=None, isPytorch=True):\n",
        "    np.random.seed(4)\n",
        "    opt = optimizer(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        start = time.time()\n",
        "        avg_acc, avg_loss = 0, 0\n",
        "        if isPytorch:\n",
        "          avg_acc, avg_loss = epoch_pytorch_cifar10(dataloader, model, loss_fn=loss_fn, opt=opt)\n",
        "        else:\n",
        "          avg_acc, avg_loss = epoch_general_cifar10(dataloader, model, loss_fn=loss_fn, opt=opt)\n",
        "        epoch_t = time.time() - start\n",
        "        print(f\"Epoch: {epoch}, Acc: {avg_acc}, Loss: {avg_loss}, Time: {epoch_t:.2f}s\")\n",
        "\n",
        "    return avg_acc, avg_loss\n",
        "\n",
        "def epoch_pytorch_cifar10(dataloader, model, loss_fn, opt=None):\n",
        "    train_loss = 0.0\n",
        "    correct = 0.0\n",
        "\n",
        "    # training model loop\n",
        "    model.train()\n",
        "    for data, labels in dataloader:\n",
        "\n",
        "      if train_on_gpu:\n",
        "        data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "      opt.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = loss_fn(output, labels)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      train_loss += loss.item() * data.size(0)\n",
        "      _, predicted = torch.max(output, 1)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct/len(dataloader.dataset), train_loss/len(dataloader.dataset)\n",
        "\n",
        "def epoch_general_cifar10(dataloader, model, loss_fn=ndl.nn.SoftmaxLoss(), opt=None):\n",
        "    np.random.seed(4)\n",
        "    correct, total_loss = 0, 0\n",
        "\n",
        "    if opt is None:\n",
        "        model.eval()\n",
        "        for batch in dataloader:\n",
        "            X, y = batch\n",
        "            X, y = ndl.Tensor(X, device=device), ndl.Tensor(y, device=device)\n",
        "            out = model(X)\n",
        "            loss = loss_fn(out, y)\n",
        "            correct += np.sum(np.argmax(out.numpy(), axis=1) == y.numpy())\n",
        "            total_loss += loss.data.numpy() * y.shape[0]\n",
        "    else:\n",
        "        model.train()\n",
        "        for batch in dataloader:\n",
        "            opt.reset_grad()\n",
        "            X, y = batch\n",
        "            X, y = ndl.Tensor(X, device=device), ndl.Tensor(y, device=device)\n",
        "            out = model(X)\n",
        "            loss = loss_fn(out, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            correct += np.sum(np.argmax(out.numpy(), axis=1) == y.numpy())\n",
        "            total_loss += loss.data.numpy() * y.shape[0]\n",
        "\n",
        "    sample_nums = len(dataloader.dataset)\n",
        "    return correct / sample_nums, total_loss / sample_nums"
      ],
      "metadata": {
        "id": "E5TUHH14Tx4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJPCEllBKGVC"
      },
      "source": [
        "####Check for GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hW4lB08K2EN",
        "outputId": "7daff265-b7a1-4f81-ff7d-1e23563711ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('PyTorch version:', torch.__version__)\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.0+cu121\n",
            "Sun Nov  3 10:43:40 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlzJAQ3jLnRt"
      },
      "source": [
        "# Set random seed for reproducability\n",
        "torch.manual_seed(271828)\n",
        "np.random.seed(271728)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PyTorch Model for Cifar10"
      ],
      "metadata": {
        "id": "qwRBmuJVUbts"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPjns4ErHkfX"
      },
      "source": [
        "####Data Preparation for PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y0XCJ6sLJe_",
        "outputId": "fe7e8494-4468-4380-bd2c-e1cb53ee0a49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# transform for the training data\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.1307], [0.3081])\n",
        "])\n",
        "\n",
        "# load datasets, downloading if needed\n",
        "train_set = CIFAR10('./data/cifar10', train=True, download=True,\n",
        "                  transform=train_transform)\n",
        "\n",
        "train_len = len(train_set)\n",
        "idxes = list(range(train_len))\n",
        "\n",
        "#shuffle the indexes\n",
        "np.random.shuffle(idxes)\n",
        "\n",
        "split = int(np.floor(0.2*train_len))\n",
        "train_idx, valid_idx = idxes[split: ], idxes[:split]\n",
        "\n",
        "#Sampler gets the batches of data\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_len = len(train_set)\n",
        "idxes = list(range(train_len))\n",
        "\n",
        "#shuffle the indexes\n",
        "np.random.shuffle(idxes)\n",
        "\n",
        "split = int(np.floor(0.2*train_len))\n",
        "train_idx, valid_idx = idxes[split: ], idxes[:split]\n",
        "\n",
        "#Sampler gets the batches of data\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=20, sampler=train_sampler, num_workers=0)\n",
        "valid_loader = torch.utils.data.DataLoader(train_set, batch_size=20, sampler=valid_sampler, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 76.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTZM9tlcXVET"
      },
      "source": [
        "###Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "              nn.BatchNorm2d(out_channels),\n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResNet9(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = conv_block(in_channels, 16)\n",
        "        self.conv2 = conv_block(16, 32, pool=True)\n",
        "        self.res1 = nn.Sequential(conv_block(32, 32), conv_block(32, 32))\n",
        "\n",
        "        self.conv3 = conv_block(32, 64, pool=True)\n",
        "        self.conv4 = conv_block(64, 128, pool=True)\n",
        "        self.res2 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "\n",
        "        self.classifier = nn.Sequential(nn.AdaptiveMaxPool2d((1,1)),\n",
        "                                        nn.Flatten(),\n",
        "                                        nn.Dropout(0.2),\n",
        "                                        nn.Linear(128, num_classes))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "model = ResNet9(3, 10)\n",
        "print(model)\n",
        "\n",
        "if train_on_gpu:\n",
        "  model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWOn6YpyZmLu",
        "outputId": "ce1ffe0d-9a22-4d1d-dcc1-0496f77fa04b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet9(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (res1): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (res2): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): AdaptiveMaxPool2d(output_size=(1, 1))\n",
            "    (1): Flatten(start_dim=1, end_dim=-1)\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "DVngbKycgLw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cifar10(model, train_loader, n_epochs=10, optimizer=optim.Adam, lr=1e-4, loss_fn=nn.CrossEntropyLoss())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtjGDBSApGY8",
        "outputId": "ce9c5f6b-346d-4a66-b14d-6b9edb7cb19e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Acc: 0.41844, Loss: 1.101134807085991, Time: 39.75s\n",
            "Epoch: 1, Acc: 0.5559, Loss: 0.6986673478364944, Time: 39.29s\n",
            "Epoch: 2, Acc: 0.61686, Loss: 0.5276486627548933, Time: 39.44s\n",
            "Epoch: 3, Acc: 0.65254, Loss: 0.4206232277840376, Time: 39.08s\n",
            "Epoch: 4, Acc: 0.68348, Loss: 0.3356303848192096, Time: 39.39s\n",
            "Epoch: 5, Acc: 0.70388, Loss: 0.2789133390113711, Time: 39.23s\n",
            "Epoch: 6, Acc: 0.7238, Loss: 0.2222105843245983, Time: 39.37s\n",
            "Epoch: 7, Acc: 0.73588, Loss: 0.18663425630070268, Time: 39.44s\n",
            "Epoch: 8, Acc: 0.74752, Loss: 0.15300494082830846, Time: 39.38s\n",
            "Epoch: 9, Acc: 0.75538, Loss: 0.1315854042828083, Time: 39.49s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.75538, 0.1315854042828083)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Custom Model for Cifar10"
      ],
      "metadata": {
        "id": "abS6FR0oUFl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to set up the assignment\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/\n",
        "!mkdir -p 10714\n",
        "%cd /content/drive/MyDrive/10714\n",
        "!git clone https://github.com/dlsys10714/hw4.git\n",
        "%cd /content/drive/MyDrive/10714/hw4\n",
        "\n",
        "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
        "!pip3 install pybind11\n",
        "\n",
        "%set_env PYTHONPATH ./python\n",
        "%set_env NEEDLE_BACKEND nd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFYgc9wahSco",
        "outputId": "ac55731b-518f-4cdd-ce73-753679f70a6a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/10714\n",
            "fatal: destination path 'hw4' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/10714/hw4\n",
            "Collecting git+https://github.com/dlsys10714/mugrade.git\n",
            "  Cloning https://github.com/dlsys10714/mugrade.git to /tmp/pip-req-build-gukw756v\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/dlsys10714/mugrade.git /tmp/pip-req-build-gukw756v\n",
            "  Resolved https://github.com/dlsys10714/mugrade.git to commit 656cdc2b7ad5a37e7a5347a7b0405df0acd72380\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mugrade\n",
            "  Building wheel for mugrade (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mugrade: filename=mugrade-1.2-py3-none-any.whl size=3935 sha256=0bd6febf117fdd0d468597ccc8170f6610c83803f92a5b97bed8b1def0e347fd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5_xywv0j/wheels/8b/ba/3a/621da1207eab160c01968c5e0bd1266f505b9e3f8010376d61\n",
            "Successfully built mugrade\n",
            "Installing collected packages: mugrade\n",
            "Successfully installed mugrade-1.2\n",
            "Collecting pybind11\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybind11\n",
            "Successfully installed pybind11-2.13.6\n",
            "env: PYTHONPATH=./python\n",
            "env: NEEDLE_BACKEND=nd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation for Custom Framework"
      ],
      "metadata": {
        "id": "EbK6Iu-MgcCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('./python')\n",
        "sys.path.append('./apps')\n",
        "import needle as ndl\n",
        "from models import ResNet9\n",
        "from simple_ml import train_cifar10, evaluate_cifar10\n",
        "\n",
        "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
        "dataloader = ndl.data.DataLoader(\\\n",
        "         dataset=dataset,\n",
        "         batch_size=128,\n",
        "         shuffle=True)"
      ],
      "metadata": {
        "id": "sxtmX4xTjrWH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model"
      ],
      "metadata": {
        "id": "-2hKoFBTgi1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet9(ndl.nn.Module):\n",
        "    def __init__(self, device=None, dtype=\"float32\"):\n",
        "        super().__init__()\n",
        "        ### BEGIN YOUR SOLUTION ###\n",
        "        self.device = device\n",
        "        self.dtype = dtype\n",
        "        self.model = self.get_model()\n",
        "        ### END YOUR SOLUTION\n",
        "\n",
        "    def ConvBN(self, a, b, k, s):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv(a, b, kernel_size = k, stride = s, device = self.device, dtype = self.dtype),\n",
        "            nn.BatchNorm2d(b, device = self.device, dtype = self.dtype),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def get_model(self):\n",
        "        return nn.Sequential(\n",
        "            self.ConvBN(3, 16, 7, 4),\n",
        "            self.ConvBN(16, 32, 3, 2),\n",
        "            nn.Residual(\n",
        "                nn.Sequential(\n",
        "                    self.ConvBN(32, 32, 3, 1),\n",
        "                    self.ConvBN(32, 32, 3, 1)\n",
        "                )\n",
        "            ),\n",
        "            self.ConvBN(32, 64, 3, 2),\n",
        "            self.ConvBN(64, 128, 3, 2),\n",
        "            nn.Residual(\n",
        "                nn.Sequential(\n",
        "                    self.ConvBN(128, 128, 3, 1),\n",
        "                    self.ConvBN(128, 128, 3, 1)\n",
        "                )\n",
        "            ),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, 128, device = self.device, dtype = self.dtype),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10, device = self.device, dtype = self.dtype)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### BEGIN YOUR SOLUTION\n",
        "        return self.model(x)\n",
        "        ### END YOUR SOLUTION\n",
        "\n",
        "device = ndl.cuda()\n",
        "model = ResNet9(device=device, dtype=\"float32\")"
      ],
      "metadata": {
        "id": "Xrn5COE6k5vO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "h76Q5sFOgnD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cifar10(model, dataloader, n_epochs=10, optimizer=ndl.optim.Adam,\n",
        "      lr=0.001, weight_decay=0.001)"
      ],
      "metadata": {
        "id": "lowzAQ1ck-oH",
        "outputId": "242978b1-7c6d-4a19-9917-c9ee9b716a11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer: <needle.optim.Adam object at 0x7e21998dc4c0>, Loss function: <needle.nn.nn_basic.SoftmaxLoss object at 0x7e2198ea0fd0>\n",
            "Epoch: 0, Acc: 0.38966, Loss: [1.6985745], Time: 76.25s\n",
            "Optimizer: <needle.optim.Adam object at 0x7e21998dc4c0>, Loss function: <needle.nn.nn_basic.SoftmaxLoss object at 0x7e2198ea0fd0>\n",
            "Epoch: 1, Acc: 0.49714, Loss: [1.3992902], Time: 76.25s\n",
            "Optimizer: <needle.optim.Adam object at 0x7e21998dc4c0>, Loss function: <needle.nn.nn_basic.SoftmaxLoss object at 0x7e2198ea0fd0>\n",
            "Epoch: 2, Acc: 0.54294, Loss: [1.2691967], Time: 76.48s\n",
            "Optimizer: <needle.optim.Adam object at 0x7e21998dc4c0>, Loss function: <needle.nn.nn_basic.SoftmaxLoss object at 0x7e2198ea0fd0>\n",
            "Epoch: 3, Acc: 0.57882, Loss: [1.1748431], Time: 76.53s\n",
            "Optimizer: <needle.optim.Adam object at 0x7e21998dc4c0>, Loss function: <needle.nn.nn_basic.SoftmaxLoss object at 0x7e2198ea0fd0>\n",
            "Epoch: 4, Acc: 0.60848, Loss: [1.099906], Time: 76.76s\n",
            "Optimizer: <needle.optim.Adam object at 0x7e21998dc4c0>, Loss function: <needle.nn.nn_basic.SoftmaxLoss object at 0x7e2198ea0fd0>\n",
            "Epoch: 5, Acc: 0.63326, Loss: [1.0367328], Time: 76.58s\n",
            "Optimizer: <needle.optim.Adam object at 0x7e21998dc4c0>, Loss function: <needle.nn.nn_basic.SoftmaxLoss object at 0x7e2198ea0fd0>\n",
            "Epoch: 6, Acc: 0.65208, Loss: [0.98273927], Time: 76.63s\n",
            "Optimizer: <needle.optim.Adam object at 0x7e21998dc4c0>, Loss function: <needle.nn.nn_basic.SoftmaxLoss object at 0x7e2198ea0fd0>\n",
            "Epoch: 7, Acc: 0.67066, Loss: [0.9310095], Time: 76.84s\n",
            "Optimizer: <needle.optim.Adam object at 0x7e21998dc4c0>, Loss function: <needle.nn.nn_basic.SoftmaxLoss object at 0x7e2198ea0fd0>\n",
            "Epoch: 8, Acc: 0.68682, Loss: [0.88785225], Time: 76.27s\n",
            "Optimizer: <needle.optim.Adam object at 0x7e21998dc4c0>, Loss function: <needle.nn.nn_basic.SoftmaxLoss object at 0x7e2198ea0fd0>\n",
            "Epoch: 9, Acc: 0.69814, Loss: [0.85183305], Time: 76.49s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.69814, array([0.85183305], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}